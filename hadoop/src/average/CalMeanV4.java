//CalMean.java
package average;

import java.util.ArrayList;
import java.util.List;
import java.util.StringTokenizer;
import java.io.IOException;


//class IntWritable represents an integer
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.DoubleWritable;
//class Text represents a string
import org.apache.hadoop.io.Text;

//import org.apache.hadoop.io.*;

//class Mapper maps the input into the output
import org.apache.hadoop.mapreduce.Mapper;
//class Reducer is the Reduce
import org.apache.hadoop.mapreduce.Reducer;

//Path class keep the path of the files or directory
import org.apache.hadoop.fs.Path;
//bring a job for the task, the job is responsible for the configuration
import org.apache.hadoop.mapreduce.Job;
//split the file
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
//write the output file
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
//parser the hadoop command parameters
import org.apache.hadoop.util.GenericOptionsParser;
//import org.apache.hadoop.util.Tool;
//import org.apache.hadoop.util.ToolRunner;
// JobConf extends Configuration
// a map/reduce job configuratjion
//import org.apache.hadoop.mapred.JobConf;
//import org.apache.hadoop.mapred.JobClient;

//the configuration
import org.apache.hadoop.conf.Configuration;
//
//ToolRunner can be used to run classes implementing Tool interface
//It works in conjunction with GenericOptionsParser to parse the generic hadoop command line arguments
//and modifies the Configuration fo the Tool
//import org.apache.hadoop.util.ToolRunner;
//
//import org.apache.hadoop.conf.Configured;

public class CalMeanV4 { 
    //Mapper<K1, V1, K2, V2>
    public static class MeanMapper extends Mapper<Object, Text, Text, IntWritable> {
        //define the output key
        private Text id = new Text();
        //define the output value
        //private IntWritable one = new IntWritable(1);

        String delims = "\n";
        //map(K1 key, V1 value, OutputCollector<K2, V2> output, Reporter reporter);
        //Each invocation of the map() method is given a key/value pair of types K1 and V1, respectively;
        //The key/value pairs generated by the mapper are outputted via the collect() method of the OutputCollector object
        public void map(Object key, Text value, Context context) throws IOException, InterruptedException {
            String line = value.toString();
            StringTokenizer st = new StringTokenizer(line, delims);

            while(st.hasMoreTokens()){
            	String tokens[] = st.nextToken().split(",");
                id.set(tokens[0]);
                context.write(id, new IntWritable( Integer.parseInt(tokens[1]) ));
            }

        }
    }

    public static class MeanReducer extends Reducer<Text, IntWritable, Text, DoubleWritable> {
        //int count = 0;
        DoubleWritable result = new DoubleWritable();
        //each invocation of the reduce() method at the reducer is given a key of type K2 and a list of values of type V2
        //The reduce() method is also given an OutputCollector to gather its key/value output, which is of type K3/V3
        public void reduce(Text key, Iterable<IntWritable> values, Context context) throws IOException,InterruptedException {
            double sum = 0.0;
            int count = 0;
            
            for(IntWritable val : values){
                sum += val.get();
                count += 1;
            }

            sum = sum / count ;
            result.set(sum);
            // in the reduce() method we can call output.collect((K3) k, (V3) v)
            context.write(key, result);
        }
        
    }

    public static void main(String[] args) throws Exception {
        // create a new JobConf
    	Configuration conf = new Configuration();
        //JobConf conf = new JobConf(getConf(), CalMeanV2.class);	
        
        String[] otherArgs = new GenericOptionsParser(conf, args).getRemainingArgs();
        List<String> other_args = new ArrayList<String>();
        /*
        if(otherArgs.length != 3){
        	System.err.println("Usage: CalMeanV4 <reducer> <in> <out> ");
        	System.exit(2); 
        } else {
        */
        	for(int i = 0; i< otherArgs.length; ++i) {
        		
        		if("-reducer".equals(otherArgs[i])){
        			other_args.add(otherArgs[++i]);
        			//args[++i] = 1;
        	} else {
        		
	    		other_args.add(otherArgs[i]);
	    	    }
        	}
        //}
        
        //each running task is a Job, and the name is "CalMean"
        Job job = new Job(conf, "CalMeanV4");
        job.setJarByClass(CalMeanV4.class);
        //job.setJobName("CalMean");

        //set the number of map tasks
        // https://www.ibm.com/developerworks/community/wikis/home?lang=en#!/wiki/W265aa64a4f21_43ee_b236_c42a1c875961/page/MapReduce%20-%20Tuning%20the%20number%20of%20map%20tasks
        //The number of map tasks for a Hadoop job is typically controlled by the input data size and the split size.
        //performance suffers if a Hadoop job creates a large number of map tasks, and most or all of those map tasks run only for a few seconds
        //
        //To reduce the number of map tasks for a Hadoop job,
        // * increase the block size.  the default value for the dfs.block.size parameter is 128 MB. Typically, each map task processes one block, or 128 MB.
        //     - If the map tasks have very short durations, we can speed the Hadoop jobs by using a larger block size and fewer map tasks.
        //     - For best performance, ensure that the dfs.block.size value matches the block size of the data that is processed on the DFS.
        //
        //*  Assign each mapper to process more data.
        //     - if the input is many small files, Hadoop jobs likely generate one map task per small file, regardless of the size of the dfs.block.size parameter.
        //     - This situation causes many map tasks, with each mapper doing very little work. Combine the small files into larger files and have the map tasks process the larger files,
        //     - resulting in fewer map tasks doing more work.
        //     - The mapreduce.input.fileinputformat.split.minsize parameter in the mapred-site.xml file specifies the minimum data input size that a map task processes.
        //     - The default value is 0. 
        //     - Assign this parameter a value that is close to the value of the dfs.block.size parameter and, as necessary, repeatedly double its value until you are satisfied
        //     - with the MapReduce behavior and performance.
        //
        //set the number of reduce tasks
        job.setNumReduceTasks(Integer.parseInt(other_args.get(0)));
        
        //setup Mapper and Reducer
        job.setMapperClass(MeanMapper.class);
       // conf.setCombinerClass(MeanReducer.class);
        job.setReducerClass(MeanReducer.class);
        
        //setup the type of key and value
        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(IntWritable.class);

        //job.setInputFormat(TextInputFormat.class);
        //job.setOutputFormat(TextOutputFormat.class);
        
        FileInputFormat.setInputPaths(job, new Path(other_args.get(1)));
        FileOutputFormat.setOutputPath(job, new Path(other_args.get(2)));
        
        System.exit(job.waitForCompletion(true) ? 0 : 1);
        //JobClient.runJob(conf);
        
        //return 0;
        //System.exit(job.waitForCompletion(true) ? 0 : 1);        
    }
    
    /*
	public static void main(String[] args) throws Exception {
		int res = ToolRunner.run(new Configuration(), new WordCountV2(), args);
		System.exit(res);
	}
	*/
    
}
